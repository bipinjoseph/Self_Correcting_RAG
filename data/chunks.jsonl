{"chunk_id": "chunk_0", "text": "RESEARCH PROPOSAL \nSynopsis  \nFor \n     Zero-Shot Sim-to-Real Physics Inference from Video \nFoundation Models \n \n \nBIPIN JOSEPH  \n24MCA107 \n \n     MCA Artificial Intelligence and Machine Learning with Minor in Data Science \nThe Yenepoya Institute of Arts, Science, Commerce & Management \nMangalore \n \n27/10/2025 \n \n \n \nUnder the guidance of  \nDr Parameshwar R Hegde  \nAssistant Professor, Department of Computer Science  \nThe Yenepoya Institute of Arts, Science, Commerce & Management  \nMangalore \nSu"}
{"chunk_id": "chunk_1", "text": "of Computer Science  \nThe Yenepoya Institute of Arts, Science, Commerce & Management  \nMangalore \nSubmitted to \n \n \n \n \n \nTHE YENEPOYA INSITUTE OF ARTS, SCIENCE, COMMERCE & \nMANAGEMENT (YIASCM) \nYENEPOYA (DEEMED TO BE UNIVERSITY) \n \nMANGALORE, KARNATAKA \n \n\nI. Title of the Project \n\u201cZero-Shot Sim-to-Real Physics Inference from Video Foundation Models\u201d \nII. Introduction \nThe accurate simulation of physical dynamics is a cornerstone of realistic digital content \ncreation (e.g., gaming, VFX) and ro"}
{"chunk_id": "chunk_2", "text": "physical dynamics is a cornerstone of realistic digital content \ncreation (e.g., gaming, VFX) and robust autonomous systems (e.g., robotics). A significant \nbottleneck in these domains is the parameterization of physical models\u2014assigning \nquantitative values for properties like elasticity or dynamic friction, which is often \nperformed manually through iterative refinement. Recent advances in Video Foundation \nModels (VFMs)\u2014large-scale neural networks pre-trained on diverse video datasets using \n"}
{"chunk_id": "chunk_3", "text": " Foundation \nModels (VFMs)\u2014large-scale neural networks pre-trained on diverse video datasets using \nobjectives like generative modeling or self-supervised learning\u2014have demonstrated \nremarkable capabilities in semantic video understanding. These models implicitly learn rich, \nhigh-dimensional representations of motion and dynamics. This project investigates the \npotential of leveraging these learned representations for quantitative physical inference: \nextracting explicit, continuous-valued phys"}
{"chunk_id": "chunk_4", "text": "ed representations for quantitative physical inference: \nextracting explicit, continuous-valued physical parameters directly from video input, thereby \nautomating a critical aspect of physically-based modeling. \n \nIII. Current Trends in the Topic \nResearch into visual physics inference spans several paradigms. Differentiable physics \nengines enable gradient-based system identification but are computationally intensive and \nrequire strong priors. Heuristic-based methods, often termed \"oracle\" app"}
{"chunk_id": "chunk_5", "text": "utationally intensive and \nrequire strong priors. Heuristic-based methods, often termed \"oracle\" approaches, use \nclassical computer vision to track explicit visual cues (e.g., trajectories, deformation) but lack \nrobustness and generalizability. Unsupervised learning via generative models (e.g., next-\nframe prediction) captures latent dynamics but yields implicit representations unsuitable for \ndirect quantitative querying. The current trend involves leveraging large-scale foundation \nmodels. W"}
{"chunk_id": "chunk_6", "text": "irect quantitative querying. The current trend involves leveraging large-scale foundation \nmodels. While benchmarks like Physion++ assess qualitative reasoning, the work by Zhan \net al. (2025) pioneers probing VFMs (both generative like DynamiCrafter and self-supervised \nlike V-JEPA-2) for  \nquantitative values. However, this work critically highlights the simulation-to-reality (Sim-\nto-Real) gap as a major impediment to real-world applicability, necessitating domain \nadaptation techniques like "}
{"chunk_id": "chunk_7", "text": "as a major impediment to real-world applicability, necessitating domain \nadaptation techniques like fine-tuning. \n \nIV. Aim of the Research \nThe primary aim of this research is to systematically investigate and quantify the implicit \nphysical knowledge encoded within the latent representations of pre-trained Video \nFoundation Models. Specifically, it seeks to develop and rigorously evaluate a methodology \nfor extracting explicit, quantitative estimates of dynamic physical properties (elasticity,"}
{"chunk_id": "chunk_8", "text": "odology \nfor extracting explicit, quantitative estimates of dynamic physical properties (elasticity, \nfriction) from raw video by probing frozen VFM feature hierarchies. A central objective is to \naddress the Sim-to-Real generalization gap by exploring the efficacy of domain \nrandomization during synthetic data generation. The goal is to achieve robust zero-shot \ninference on real-world videos, thereby obviating the need for fine-tuning on target domain \ndata, which was a limitation in prior wor"}
{"chunk_id": "chunk_9", "text": "ereby obviating the need for fine-tuning on target domain \ndata, which was a limitation in prior work . \nV. Problem statement  \nThe core problem is the inefficiency and lack of empirical grounding in current workflows \nfor parameterizing physically-based simulations. Existing automated physics inference \nmethods are either domain-specific, computationally prohibitive, or yield non-interpretable \nresults. While VFMs offer a promising avenue due to their rich learned representations of \ndynamics, "}
{"chunk_id": "chunk_10", "text": "esults. While VFMs offer a promising avenue due to their rich learned representations of \ndynamics, extracting quantitative physical parameters remains an \"underexplored\" \nchallenge. Furthermore, models trained purely on synthetic data \"struggle to generalize\" to \nreal-world videos, requiring domain adaptation. This leads to the specific research question: \nCan domain randomization applied during synthetic video generation enable a VFM-\nbased probing architecture (utilizing a frozen backbone and"}
{"chunk_id": "chunk_11", "text": "synthetic video generation enable a VFM-\nbased probing architecture (utilizing a frozen backbone and a lightweight trainable \nhead) to perform accurate quantitative physical inference on real-world videos in a \nzero-shot manner, significantly mitigating the Sim-to-Real gap observed in baseline \napproaches? \n \nVI. Research Methodology \nThe research will employ a structured, empirical methodology: \n1. Synthetic Dataset Generation with Domain Randomization: A labelled dataset \n(PhysVid-style ) will"}
{"chunk_id": "chunk_12", "text": "1. Synthetic Dataset Generation with Domain Randomization: A labelled dataset \n(PhysVid-style ) will be generated using the Blender physics engine and its Python \nscripting API (bpy). Videos depicting canonical physical scenarios will be created. \nCrucially, domain randomization will be applied by programmatically varying \nnuisance parameters per video, including camera viewpoint (pose and potentially \nintrinsics), object appearance (randomized PBR textures, albedo), lighting conditions \n(HDRI m"}
{"chunk_id": "chunk_13", "text": "ally \nintrinsics), object appearance (randomized PBR textures, albedo), lighting conditions \n(HDRI maps, light source properties), and initial object states, following principles \noutlined in arXiv:2510.02311's appendix . This forces the model to learn \nrepresentations invariant to visual style. \n2. Model Architecture: The inference model employs a frozen VFM backbone (e.g., V-\nJEPA-2, a ViT-based architecture ) accessed via Hugging Face transformers. A \nlightweight, trainable probing module imp"}
{"chunk_id": "chunk_14", "text": " architecture ) accessed via Hugging Face transformers. A \nlightweight, trainable probing module implemented in PyTorch will perform the \ninference task. This module comprises: \n\u2022 A learnable query vector q: This vector parameterizes the specific physical \nproperty being queried. \n\u2022 A cross-attention mechanism: This compute attention scores between q and \nthe VFM's spatiotemporal feature tokens (r\u1d62) from multiple layers, allowing q \nto selectively pool relevant features: P = \u03a3 softmax(q\u00b7r\u1d62)\u00b7r\u1d62. "}
{"chunk_id": "chunk_15", "text": "\u1d62) from multiple layers, allowing q \nto selectively pool relevant features: P = \u03a3 softmax(q\u00b7r\u1d62)\u00b7r\u1d62. This isolates task-\nspecific information from the general-purpose VFM features. \n\u2022 An MLP Regression Head: A multi-layer perceptron maps the aggregated \nfeature vector P to the final scalar prediction. \n3. Training Protocol: Only the probing module parameters will be optimized using \nsupervised learning on the synthetic dataset via the Adam optimizer. Following Zhan \net al. (2025) , the loss funct"}
{"chunk_id": "chunk_16", "text": "ning on the synthetic dataset via the Adam optimizer. Following Zhan \net al. (2025) , the loss function will be L1 loss for elasticity/friction and Log L1 loss \nfor viscosity (to handle its large dynamic range). \n4. Evaluation Protocol: Model performance will be quantitatively assessed using the \nPearson Correlation Coefficient on distinct test splits: test-1 (in-distribution \nsynthetic), test-2 (out-of-distribution synthetic), and test-3 (real-world videos) . \nPearson correlation effectively me"}
{"chunk_id": "chunk_17", "text": "out-of-distribution synthetic), and test-3 (real-world videos) . \nPearson correlation effectively measures the ordinal correctness and linear \nrelationship, crucial for evaluating generalization across domains where absolute \nscales might shift. Success criteria include high correlation on synthetic data and \nsignificantly improved zero-shot correlation on test-3 compared to a non-domain-\nrandomized baseline. \n5. Interpretability: Attention maps derived from the cross-attention weights will be \n"}
{"chunk_id": "chunk_18", "text": "ed baseline. \n5. Interpretability: Attention maps derived from the cross-attention weights will be \nvisualized overlayed on video frames. This provides qualitative insight into the \nmodel's spatial-temporal focus when inferring specific physical properties. \n \nVII. Conclusion \nThis project proposes a systematic investigation into extracting quantitative physical \nknowledge from Video Foundation Models. By implementing a state-of-the-art probing \nmethodology and introducing domain randomization a"}
{"chunk_id": "chunk_19", "text": "dels. By implementing a state-of-the-art probing \nmethodology and introducing domain randomization as a targeted enhancement to address \nthe critical Sim-to-Real challenge, this research aims to demonstrate the feasibility of robust, \nzero-shot physical parameter estimation from video. The expected outcomes include a \nnovel, domain-randomized dataset, rigorous empirical validation of the proposed \nmethodology's effectiveness in improving real-world generalization, and a functional \nprototype. Su"}
{"chunk_id": "chunk_20", "text": "\nmethodology's effectiveness in improving real-world generalization, and a functional \nprototype. Successful completion will contribute significant insights into the emergent \nphysical reasoning capabilities of VFMs and offer a practical advancement towards \nautomating physically realistic digital content creation and enhancing robotic perception, \npotentially paving the way for more physically grounded AI systems capable of deeper \ninteraction with the physical world. \n"}
{"chunk_id": "chunk_21", "text": "ounded AI systems capable of deeper \ninteraction with the physical world. \n"}
